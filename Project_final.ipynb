{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "G2OId-KLsRB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel"
      ],
      "metadata": {
        "id": "0w694GYdsUwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.ru.examples import sentences\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import re\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "3heWLDu2sU83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "additional__symbols = [\"...\", \"``\", \"--\", \"''\", \"’\", \"-=-\", \"—\", \"....\", \"“\", \"”\"]\n",
        "\n",
        "custom_punctuation = string.punctuation\n",
        "for i in additional__symbols:\n",
        "    custom_punctuation += i\n",
        "\n",
        "custom_punctuation"
      ],
      "metadata": {
        "id": "gp7-MNx4sZ1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newStopWords = [\"'s\",\"'ll\", \"'ve\", \"'d\", \"'n't\", \"n't\", \"'m\", \"'re\"]\n",
        "for i in newStopWords:\n",
        "    stop_words.add(i)"
      ],
      "metadata": {
        "id": "KdTTgdvJso6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "id": "XfaSlXTlMaKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "QbGA50Z0s2jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        book_text = f.read()\n",
        "\n",
        "    book_text = book_text.lower()\n",
        "    tokens = word_tokenize(book_text)\n",
        "    tokens = [token for token in tokens if token not in custom_punctuation]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = []\n",
        "    for word in tokens:\n",
        "        pos = get_wordnet_pos(word)\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "        lemmatized_words.append(lemma)\n",
        "    lemmas = ' '.join(lemmatized_words)\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(lemmas)\n",
        "    return lemmas\n",
        "\n",
        "input_file = 'harry_and_prince.txt'\n",
        "output_file = 'lemmas_final_harry.txt'\n",
        "final = lemmatize(input_file, output_file)\n",
        "print(\"Lemmatization is completed. Lemmatized text saved to\", output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SHI1eKmsqe8",
        "outputId": "bd441319-d2ce-49e7-ed3c-fbd5f1824765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization is completed. Lemmatized text saved to lemmas_final_harry.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final.split()[25:40]"
      ],
      "metadata": {
        "id": "FcAnGfVWg1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The most frequent tokens**"
      ],
      "metadata": {
        "id": "zfpZDvMz3dPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lemmas_final_harry.txt\", encoding='utf-8') as txt:\n",
        "    text = txt.read()\n",
        "    frequencies_list = nltk.FreqDist(text.split())\n",
        "\n",
        "    filtered_word_freq = dict((word, freq) for word, freq in frequencies_list.items() if not word.isdigit())\n",
        "\n",
        "print(filtered_word_freq)"
      ],
      "metadata": {
        "id": "Pjko3O57tRPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequencies_list.most_common(10)"
      ],
      "metadata": {
        "id": "DNbLyTozvf2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_word_frequency(words,top_n=10):\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    labels = [element[0] for element in word_freq.most_common(top_n)]\n",
        "    counts = [element[1] for element in word_freq.most_common(top_n)]\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.title(\"The most frequent tokens\")\n",
        "    plt.ylabel(\"Number\")\n",
        "    plt.xlabel(\"Token\")\n",
        "    plot = sns.barplot(x=labels, y=counts)\n",
        "    return plot"
      ],
      "metadata": {
        "id": "o8JF3WJM3oDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_word_frequency(text.split())"
      ],
      "metadata": {
        "id": "6-jgkBRA3rIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The most frequent tokens without the most frequent words in English**"
      ],
      "metadata": {
        "id": "_mQpNmJ64gVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = [\n",
        "    \"want\",\n",
        "    \"have\",\n",
        "    \"do\",\n",
        "    \"look\",\n",
        "    \"think\",\n",
        "    \"give\",\n",
        "    \"turn\",\n",
        "    \"say\",\n",
        "    \"get\",\n",
        "    \"make\",\n",
        "    \"like\",\n",
        "    \"seem\",\n",
        "    \"go\",\n",
        "    \"know\",\n",
        "    \"come\",\n",
        "    \"take\",\n",
        "    \"ask\",\n",
        "    \"see\"\n",
        "]\n",
        "\n",
        "nouns = [\n",
        "    \"time\",\n",
        "    \"person\",\n",
        "    \"year\",\n",
        "    \"way\",\n",
        "    \"day\",\n",
        "    \"thing\",\n",
        "    \"man\",\n",
        "    \"world\",\n",
        "    \"life\",\n",
        "    \"hand\",\n",
        "    \"eye\",\n",
        "    \"face\"\n",
        "]\n",
        "\n",
        "adjectives = [\"good\", \"new\", \"first\", \"last\", \"long\", \"great\", \"little\", \"own\", \"other\", \"old\"]\n",
        "\n",
        "names = [\n",
        "    \"harry\",\n",
        "    \"potter\",\n",
        "    \"hermione\",\n",
        "    \"ron\",\n",
        "    \"professor\"\n",
        "]\n",
        "\n",
        "numerals = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten']\n",
        "\n",
        "adverbs = ['also', 'always', 'often', 'never', 'sometimes', 'very', 'well', 'here', 'there', 'now',\n",
        "           'around', 'ago', 'still', 'just', 'yet', 'already', 'even', 'almost', 'though', 'back']\n",
        "\n",
        "keys_to_delete = []\n",
        "keys_to_delete.extend(adjectives)\n",
        "keys_to_delete.extend(nouns)\n",
        "keys_to_delete.extend(verbs)\n",
        "keys_to_delete.extend(names)\n",
        "keys_to_delete.extend(numerals)\n",
        "keys_to_delete.extend(adverbs)\n"
      ],
      "metadata": {
        "id": "DnZ0ZP1k7xoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_without_common(dictionary):\n",
        "    for key, value in list(filtered_word_freq1.items()):\n",
        "        if key in keys_to_delete:\n",
        "            del filtered_word_freq1[key]\n",
        "\n",
        "    sorted_data = sorted(filtered_word_freq1.items(), key=lambda x: x[1], reverse=True)\n",
        "    labels = [element[0] for element in sorted_data[:10]]\n",
        "    counts = [element[1] for element in sorted_data[:10]]\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.title(\"The most frequent tokens without meaningless ones\")\n",
        "    plt.ylabel(\"Number\")\n",
        "    plt.xlabel(\"Token\")\n",
        "    plot = sns.barplot(x=labels, y=counts)\n",
        "\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "EcEs8iwrMH8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_word_freq1 = filtered_word_freq\n",
        "\n",
        "final = correlation_without_common(filtered_word_freq1)\n",
        "final"
      ],
      "metadata": {
        "id": "fnPP-22zMl81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation of the most frequent tokens**"
      ],
      "metadata": {
        "id": "BpA3s_r-_dgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation(input_file):\n",
        "     with open(input_file, encoding='utf-8') as txt:\n",
        "        filtered_tokens = txt.read()\n",
        "\n",
        "#create a Counter object from the list of filtered tokens, which counts the frequency of each token in the text\n",
        "        filtered_tokens = filtered_tokens.split()\n",
        "        word_freq = Counter(filtered_tokens)\n",
        "#extract the num_common_words most common tokens from the counter using the most_common method\n",
        "        num_common_words = 10\n",
        "        most_common_words = word_freq.most_common(num_common_words)\n",
        "#initialize a matrix with dimensions num_common_words x num_common_words to store the co-occurrence counts between tokens.\n",
        "        co_occurrence_matrix = np.zeros((num_common_words, num_common_words), dtype=int)\n",
        "# create a dictionary word_to_index to map each token to its index in the most common words list\n",
        "        word_to_index = {word: i for i, (word, _) in enumerate(most_common_words)}\n",
        "\n",
        "#iterate through the list of tokens and update the co-occurrence matrix for consecutive token pairs.\n",
        "# the co-occurrence count is incremented in the matrix.\n",
        "# compute the correlation matrix using np.corrcoef based on the co-occurrence matrix.\n",
        "        for i in range(len(filtered_tokens) - 1):\n",
        "            if filtered_tokens[i] in word_to_index and filtered_tokens[i + 1] in word_to_index:\n",
        "                index_i = word_to_index[filtered_tokens[i]]\n",
        "                index_j = word_to_index[filtered_tokens[i + 1]]\n",
        "                co_occurrence_matrix[index_i][index_j] += 1\n",
        "                co_occurrence_matrix[index_j][index_i] += 1\n",
        "\n",
        "        correlation_matrix = np.corrcoef(co_occurrence_matrix)\n",
        "        plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')\n",
        "        plt.colorbar()\n",
        "        plt.xticks(np.arange(num_common_words), [word for word, _ in most_common_words], rotation=45)\n",
        "        plt.yticks(np.arange(num_common_words), [word for word, _ in most_common_words])\n",
        "        plt.title('Correlation between Most Common Tokens')\n",
        "\n",
        "        return plt.show()"
      ],
      "metadata": {
        "id": "iN8VGcTMfjm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"lemmas_final_harry.txt\"\n",
        "\n",
        "final = correlation(input_file)\n",
        "final"
      ],
      "metadata": {
        "id": "NFPrSzjegRm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic Recognition**"
      ],
      "metadata": {
        "id": "bHrmWiJeAKHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics = {\n",
        "    \"Voldemort's Return\": [\"voldemort\", \"deatheaters\", \"dark magic\", \"Snape\",\n",
        "                           \"Malfoy\", \"death Eaters\", \"tom riddle\",\n",
        "                           \"vanishing cabinet\", \"resurgence\", \"half-blood prince\",\n",
        "                           \"evil\", \"unforgivable curses\", \"avada Kedavra\",\n",
        "                           \"inferi\", \"sectumsempra\", \"nagini\", \"forbidden forest\",\n",
        "                           \"dark mark\", \"kill\", \"the room of requirement\"],\n",
        "    \"Dumbledore's Secrets and Mentorship\": [\"dumbledore\", \"mentorship\", \"pensieve\",\n",
        "                                            \"memories\", \"memory\", \"past\", \"lesson\",\n",
        "                                            \"headmaster\", \"the order of the phoenix\",\n",
        "                                            \"thestrals\", \"phoenix\", \"prophecy\", \"the chosen one\"],\n",
        "    \"Romantic Relationships\": [\"snog\", \"date\", \"relationships\", \"kiss\", \"love\",\n",
        "                               \"dating\", \"ginny\", \"lavender brown\", \"cormac mclaggen\",\n",
        "                               \"dean thomas\"],\n",
        "    \"Horcruxes\": [\"horcruxes\", \"horcrux\", \"immortality\", \"soul\", \"destroy\",\n",
        "                  \"diary\", \"ring\", \"lake\", \"cave\", \"nagini\", \"orphanage\",\n",
        "                  \"Slughorn's memory\", \"Slughorn\", \"felix felicis\", \"felix\"],\n",
        "    \"Wizarding World Politics\": [\"ministry of magic\", \"minister\", \"fudge\",\n",
        "                                 \"scrimgeour\", \"azkaban\", \"muggles\", \"daily prophet\",\n",
        "                                 \"quibbler\", \"wizengamot\"],\n",
        "    \"Loss and Sacrifice\": [\"loss\", \"sacrifice\", \"death\", \"funeral\", \"tragedy\",\n",
        "                           \"sirius\", \"dumbledore\"],\n",
        "    \"Magic and Education\": [\"hogwarts\", \"magic\", \"lesson\", \"spells\", \"classes\",\n",
        "                            \"professor\", \"student\", \"quidditch\", \"transfiguration\",\n",
        "                            \"potions\", \"cauldron\", \"wand\",\n",
        "                            \"defense against the dark arts\", \"charms\", \"apparition\", \"apparate\", ],\n",
        "}\n",
        "with open('harry_and_prince.txt', 'r', encoding = \"utf-8\") as f:\n",
        "    text = f.read()\n",
        "    text = text.lower()\n",
        "topic_freq = {topic: 0 for topic in topics}\n",
        "\n",
        "for word in text.lower().split():\n",
        "    for topic, keywords in topics.items():\n",
        "        if word in keywords:\n",
        "            topic_freq[topic] += 1\n",
        "\n",
        "topics = []\n",
        "numbers = []\n",
        "for topic, freq in topic_freq.items():\n",
        "    topics.append(topic)\n",
        "    numbers.append(freq)\n",
        "    # print(f\"{topic}: {freq} keywords found\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(topics, numbers, color='skyblue')\n",
        "plt.xlabel('Topics')\n",
        "plt.ylabel('Keyword Counts')\n",
        "plt.title('Keyword Counts for Different Topics')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOe1gUslAOPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting into chapters**"
      ],
      "metadata": {
        "id": "1ZUDbQggAwo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"harry_and_prince.txt\", encoding='utf-8') as txt:\n",
        "    book_text = txt.read()\n",
        "\n",
        "chapters = re.split(r'Chapter \\d+: ', book_text)[1:]"
      ],
      "metadata": {
        "id": "pPrRc2fFArsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Frequency-based sammarizer**"
      ],
      "metadata": {
        "id": "3HQYGR0yAgbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The book has {len(chapters)} chapters\")\n",
        "print(f\"Enter the number of the chapter you want a summary of\")\n",
        "\n",
        "number = int(input())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O744KkoAOfO",
        "outputId": "cabd17f1-dc1d-49cd-971d-7530636b508e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The book has 30 chapters\n",
            "Enter the number of the chapter you want a summary of\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarizer(text):\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    #importing stopwords from NLTK and tokenizing the input text, excluding stopwords and custom punctuation marks\n",
        "    #creating a frequency table using a Counter object\n",
        "\n",
        "    freqTable = Counter([w.lower() for w in words if w.lower() not in stopWords and w.lower() not in custom_punctuation])\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentenceValue = dict()\n",
        "\n",
        "# iterating through each sentence and assigning a value based on the frequency of important words present in the sentence.\n",
        "# the value of each sentence is the sum of frequencies of important words it contains.\n",
        "    for sentence in sentences:\n",
        "        for word, freq in freqTable.items():\n",
        "            if word in sentence.lower():\n",
        "                if sentence in sentenceValue:\n",
        "                    sentenceValue[sentence] += freq\n",
        "                else:\n",
        "                    sentenceValue[sentence] = freq\n",
        "    # calculating the average value of sentences ased on the sum of values of all sentences\n",
        "    # divided by the number of sentences.\n",
        "    # selecting sentences with values greater than 1.7 times the average value.\n",
        "\n",
        "    sumValues = 0\n",
        "    for sentence in sentenceValue:\n",
        "        sumValues += sentenceValue[sentence]\n",
        "\n",
        "    average = sumValues / len(sentenceValue)\n",
        "    summary = ''\n",
        "    i = 0\n",
        "    for sentence in sentences:\n",
        "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.7 * average)):\n",
        "            i += 1\n",
        "            summary += \" \" + sentence\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "mR0CMFjtM79-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = number - 1\n",
        "\n",
        "final = summarizer(chapters[n])\n",
        "\n",
        "print(final)"
      ],
      "metadata": {
        "id": "zcMfZWu8BFHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0tc9EYVNdik",
        "outputId": "148423c1-4242-4bca-af8c-2b6a287a7627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10745"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chapters[n])"
      ],
      "metadata": {
        "id": "jDETi0QbNee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1951abb4-7a3d-47d1-ff92-a23daf30c48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29598"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Cloud**"
      ],
      "metadata": {
        "id": "iujnYAFfCprP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lemmas_final_harry.txt\", 'r', encoding='utf-8') as f:\n",
        "        book_text = f.read()"
      ],
      "metadata": {
        "id": "UmaqsPJ4Cruj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width = 1500,\n",
        "                      height = 1000,\n",
        "                      background_color='pink',\n",
        "                      colormap='Pastel1').generate(book_text)\n",
        "plt.figure(figsize=(40, 30))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "erYpSMfbCs-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KeyWord Extraction**"
      ],
      "metadata": {
        "id": "KKk1n68VC-yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = chapters\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10, stop_words='english')\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tfidf_scores = tfidf_matrix.toarray()\n",
        "\n",
        "top_keywords_per_document = []\n",
        "for i, scores in enumerate(tfidf_scores):\n",
        "    top_indices = scores.argsort()[-5:][::-1]\n",
        "    top_keywords = [feature_names[idx] for idx in top_indices]\n",
        "    top_keywords_per_document.append(top_keywords)\n",
        "\n",
        "for i, keywords in enumerate(top_keywords_per_document):\n",
        "    print(f\"Chapter {i+1}: {', '.join(keywords)}\")"
      ],
      "metadata": {
        "id": "5VTuFhKuCuTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntaxis and children**"
      ],
      "metadata": {
        "id": "l_hhb_YhbotT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def table_of_children(input_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        book_text = f.read()\n",
        "        doc = nlp_en(book_text)\n",
        "        tokens_list = []\n",
        "        numbers_list = []\n",
        "        children_list = []\n",
        "        for token in doc:\n",
        "            tokens_list.append(token)\n",
        "            children = [child for child in token.children]\n",
        "            children = [child for child in children if str(child)[0].isalpha()]\n",
        "            numbers_list.append(len([child for child in children]))\n",
        "            children_list.append(children)\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "        'Token': tokens_list,\n",
        "        'Number of children': numbers_list,\n",
        "        \"Children\": children_list})\n",
        "\n",
        "        answer = df.sort_values(by='Number of children', ascending=False)\n",
        "        return answer.head(10)\n",
        "\n",
        "input_file = 'harry_and_prince.txt'\n",
        "final = table_of_children(input_file)"
      ],
      "metadata": {
        "id": "8I8a6uhGcNHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final"
      ],
      "metadata": {
        "id": "LW5t_yzphOyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntactic Analysis**"
      ],
      "metadata": {
        "id": "OMBiHVAhdmJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syntatic_analysis(sentence):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(sentence)\n",
        "    displacy.render(doc, style=\"dep\", options={\"compact\": True, \"distance\": 100})"
      ],
      "metadata": {
        "id": "JMN1hL1medzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter a sentence\")\n",
        "text = input()\n",
        "\n",
        "\n",
        "final = syntatic_analysis(text)\n",
        "final"
      ],
      "metadata": {
        "id": "WTYZb4O7ezii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Morphology**"
      ],
      "metadata": {
        "id": "AAabvHE2OGA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def morphology(input_file):\n",
        "    with open(input_file, encoding='utf-8') as f:\n",
        "            book_text = f.read()\n",
        "    doc = nlp_en(book_text)\n",
        "\n",
        "    lst_tokens = []\n",
        "    lst_pos = []\n",
        "    lst_morph = []\n",
        "\n",
        "    for token in doc:\n",
        "        lst_tokens.append(token.text)\n",
        "        lst_pos.append(token.pos_)\n",
        "        lst_morph.append(token.morph)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "                'Token': lst_tokens,\n",
        "                'POS': lst_pos,\n",
        "                'Morphology': lst_morph}\n",
        "                    )\n",
        "\n",
        "    return df.head(10)"
      ],
      "metadata": {
        "id": "CytkL17gOZC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"harry_and_prince.txt\"\n",
        "\n",
        "final = morphology(input_file)\n",
        "final"
      ],
      "metadata": {
        "id": "dfSw2AlgOjfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS**"
      ],
      "metadata": {
        "id": "UBYinXRsPQ6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parts_of_speech(input_file):\n",
        "    lst_pos = []\n",
        "\n",
        "    with open(input_file, encoding='utf-8') as f:\n",
        "            book_text = f.read()\n",
        "\n",
        "    book_text = re.sub('[^\\w ]+', '', book_text)\n",
        "    doc = nlp_en(book_text.lower())\n",
        "\n",
        "    for token in doc:\n",
        "        lst_pos.append(token.pos_)\n",
        "\n",
        "    final_pos1 = set(lst_pos)\n",
        "    sum_ = len(lst_pos)\n",
        "    final_pos1 = sorted(list(set(lst_pos)))\n",
        "    sorted(final_pos1)\n",
        "\n",
        "    final_count1 = []\n",
        "    final_part1 = []\n",
        "    for part_of_speech in final_pos1:\n",
        "        final_count1.append(lst_pos.count(part_of_speech))\n",
        "        final_part1.append(lst_pos.count(part_of_speech) / sum_)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "            'Part of speech': final_pos1,\n",
        "            'Number': final_count1,\n",
        "            \"Ratio\": final_part1})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "oGKg_5aaOnZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"harry_and_prince.txt\"\n",
        "\n",
        "final = parts_of_speech(input_file)\n",
        "final"
      ],
      "metadata": {
        "id": "qntZZxKYPoZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER**"
      ],
      "metadata": {
        "id": "LG-4y4eoQIxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "with open('harry_and_prince.txt', 'r', encoding = \"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "K_z6pxHKPvmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_entities = []\n",
        "entities = set()\n",
        "for ent in doc.ents:\n",
        "    entities.add(ent.label_)\n",
        "    all_entities.append(ent.label_)\n",
        "\n",
        "entities = sorted(entities)"
      ],
      "metadata": {
        "id": "EtpbSxkwQZXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(entities)"
      ],
      "metadata": {
        "id": "VlTaNhLyQaiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb82e17-9042-4251-c3ab-da15349e9efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'CARDINAL': 1, 'DATE': 1, 'EVENT': 1, 'FAC': 1, 'GPE': 1, 'LAW': 1, 'LOC': 1, 'NORP': 1, 'ORDINAL': 1, 'ORG': 1, 'PERSON': 1, 'PRODUCT': 1, 'QUANTITY': 1, 'TIME': 1, 'WORK_OF_ART': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entity_counter = Counter(all_entities)\n",
        "plt.bar(entity_counter.keys(), entity_counter.values())\n",
        "plt.xlabel(\"Entity Type\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency of Named Entity Types\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n0RnjntrQb9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_names = Counter([ent.text for ent in doc.ents if ent.label_ == \"PERSON\"])\n",
        "\n",
        "top_names = all_names.most_common(20)\n",
        "\n",
        "names, name_counts = zip(*top_names)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(names, name_counts, color='skyblue')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Names')\n",
        "plt.title('Top 10 Most Common Names')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HRGWafKAQdUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BookNLP**"
      ],
      "metadata": {
        "id": "LT4_P8lCaXsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install booknlp"
      ],
      "metadata": {
        "id": "ErPTPpVqabW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##from booknlp.booknlp import BookNLP\n",
        "#import pandas as pd\n",
        "#from pprint import pprint"
      ],
      "metadata": {
        "id": "e0LSfrXhahET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_params={\n",
        "\t\t#\"pipeline\":\"entity,quote,supersense,event,coref\",\n",
        "\t\t#\"model\":\"big\"\n",
        "#}\n",
        "\n",
        "#booknlp=BookNLP(\"en\", model_params)"
      ],
      "metadata": {
        "id": "BZuWWKKRapBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####input_file = \"harry_and_prince.txt\"\n",
        "###output_directory = \"HarryPotter\"\n",
        "##book_id = \"harry\"\n",
        "\n",
        "#booknlp.process(input_file, output_directory, book_id)"
      ],
      "metadata": {
        "id": "_wUphxZXa2rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import csv\n",
        "# wide future warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRz3Z8dBa8BR",
        "outputId": "2713bd47-1ca8-430a-f659-15565180617d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls HarryPotter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9t01Hy6a-4x",
        "outputId": "522b6411-9868-4080-ecaf-d0932195a48f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "harry.book  harry.book.html  harry.entities  harry.quotes  harry.supersense  harry.tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of roles or attributes associated with characters (or entities) identified in a text**"
      ],
      "metadata": {
        "id": "jWJ2EXkJkSnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('HarryPotter/harry.book') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "len(data[\"characters\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdMqQVCvlitC",
        "outputId": "2554ce51-e4a4-473d-90c1-a28747684bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1369"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data[\"characters\"]\n",
        "data[\"characters\"][0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1fb0171lq1g",
        "outputId": "32d3f418-85b4-4a5e-b32b-60d689a3c19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['agent', 'patient', 'mod', 'poss', 'id', 'g', 'count', 'mentions'])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Agent**: An agent is typically a character or entity that performs an action in a narrative, usually sb who is actively engaged in actions or events within the text.\n",
        "\n",
        "\n",
        "\n",
        "*   **Patient**: A patient is an entity that undergoes an action or is affected by it.\n",
        "\n",
        "\n",
        "*   **Mod**: Modifiers are words or phrases that provide additional information about other elements in a sentence. In the context of characters, modifiers may indicate attributes, qualities, or relationships associated with the character.\n",
        "\n",
        "*   **Poss**: Possessors indicate ownership or possession of\n",
        "something by a character."
      ],
      "metadata": {
        "id": "fBMLHdTfkkkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokens**"
      ],
      "metadata": {
        "id": "LjIJ2k0EltWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = pd.read_csv('HarryPotter/harry.tokens', delimiter=\"\\t\", engine=\"python\", encoding='utf-8', error_bad_lines=False)\n"
      ],
      "metadata": {
        "id": "GnqwtNlydPcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.head(20)"
      ],
      "metadata": {
        "id": "ca-37EGLde4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entities**"
      ],
      "metadata": {
        "id": "OR9mt8azlwc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entities = pd.read_csv(\"HarryPotter/harry.entities\", delimiter=\"\\t\")\n",
        "entities.head(10)"
      ],
      "metadata": {
        "id": "as8vDr10dlOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quote = pd.read_csv(\"HarryPotter/harry.quotes\", delimiter=\"\\t\", on_bad_lines='skip')\n",
        "quote.head(10)"
      ],
      "metadata": {
        "id": "dqhrI84EeELK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hagrid_quotes_ = quote[quote['mention_phrase'] == \"Hagrid\"]"
      ],
      "metadata": {
        "id": "VNsFrX6VeiaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hagrid_quotes_"
      ],
      "metadata": {
        "id": "SuAKGT5mepRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyzing characters**"
      ],
      "metadata": {
        "id": "2s_y-qp4l47h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code source for the next cell: https://medium.com/bitgrit-data-science-publication/booknlp-analyze-your-favorite-books-with-nlp-606367aa0930"
      ],
      "metadata": {
        "id": "yQlNLzzTiOmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def proc(filename):\n",
        "    with open(filename) as file:\n",
        "        data=json.load(file)\n",
        "    return data\n",
        "\n",
        "def get_counter_from_dependency_list(dep_list):\n",
        "    counter=Counter()\n",
        "    for token in dep_list:\n",
        "        term=token[\"w\"]\n",
        "        tokenGlobalIndex=token[\"i\"]\n",
        "        counter[term]+=1\n",
        "    return counter\n",
        "\n",
        "\n",
        "def create_character_data(data, printTop):\n",
        "    character_data = {}\n",
        "    for character in data[\"characters\"]:\n",
        "\n",
        "        agentList=character[\"agent\"]\n",
        "        patientList=character[\"patient\"]\n",
        "        possList=character[\"poss\"]\n",
        "        modList=character[\"mod\"]\n",
        "\n",
        "        character_id=character[\"id\"]\n",
        "        count=character[\"count\"]\n",
        "\n",
        "        referential_gender_distribution=referential_gender_prediction=\"unknown\"\n",
        "\n",
        "        if character[\"g\"] is not None and character[\"g\"] != \"unknown\":\n",
        "            referential_gender_distribution=character[\"g\"][\"inference\"]\n",
        "            referential_gender=character[\"g\"][\"argmax\"]\n",
        "\n",
        "        mentions=character[\"mentions\"]\n",
        "        proper_mentions=mentions[\"proper\"]\n",
        "        max_proper_mention=\"\"\n",
        "\n",
        "        #Let's create some empty lists that we can append to.\n",
        "        poss_items = []\n",
        "        agent_items = []\n",
        "        patient_items = []\n",
        "        mod_items = []\n",
        "\n",
        "        # just print out information about named characters\n",
        "        if len(mentions[\"proper\"]) > 0:\n",
        "            max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
        "            for k, v in get_counter_from_dependency_list(possList).most_common(printTop):\n",
        "                poss_items.append((v,k))\n",
        "\n",
        "            for k, v in get_counter_from_dependency_list(agentList).most_common(printTop):\n",
        "                agent_items.append((v,k))\n",
        "\n",
        "            for k, v in get_counter_from_dependency_list(patientList).most_common(printTop):\n",
        "                patient_items.append((v,k))\n",
        "\n",
        "            for k, v in get_counter_from_dependency_list(modList).most_common(printTop):\n",
        "                mod_items.append((v,k))\n",
        "\n",
        "            # print(character_id, count, max_proper_mention, referential_gender)\n",
        "            character_data[character_id] = {\"id\": character_id,\n",
        "                                  \"count\": count,\n",
        "                                  \"max_proper_mention\": max_proper_mention,\n",
        "                                  \"referential_gender\": referential_gender,\n",
        "                                  \"possList\": poss_items,\n",
        "                                  \"agentList\": agent_items,\n",
        "                                  \"patientList\": patient_items,\n",
        "                                  \"modList\": mod_items\n",
        "                                 }\n",
        "\n",
        "    return character_data\n",
        "\n",
        "\n",
        "def find_verb_usage(character_data, analysis=[\"agent\", \"patient\"]):\n",
        "    new_analysis = []\n",
        "    for item in analysis:\n",
        "        if item == \"agent\":\n",
        "            new_analysis.append(\"agentList\")\n",
        "        elif item == \"patient\":\n",
        "            new_analysis.append(\"patientList\")\n",
        "    main_agents = {}\n",
        "    main_patients = {}\n",
        "    for character in character_data:\n",
        "        temp_data = character_data[character]\n",
        "        for item in new_analysis:\n",
        "            for verb in temp_data[item]:\n",
        "                verb = verb[1].lower()\n",
        "                if item == \"agentList\":\n",
        "                    if verb not in main_agents:\n",
        "                        main_agents[verb] = [(character, temp_data[\"max_proper_mention\"])]\n",
        "                    else:\n",
        "                        main_agents[verb].append((character, temp_data[\"max_proper_mention\"]))\n",
        "                elif item == \"patientList\":\n",
        "                    if verb not in main_patients:\n",
        "                        main_patients[verb] = [(character, temp_data[\"max_proper_mention\"])]\n",
        "                    else:\n",
        "                        main_patients[verb].append((character, temp_data[\"max_proper_mention\"]))\n",
        "    verb_usage = {\"agent\": main_agents,\n",
        "                 \"patient\": main_patients}\n",
        "    return verb_usage"
      ],
      "metadata": {
        "id": "J7WyRopSfK05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = proc(\"HarryPotter/harry.book\")"
      ],
      "metadata": {
        "id": "poBITmSTfLk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_data = create_character_data(data, 20)"
      ],
      "metadata": {
        "id": "FzEl22UjfONp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualisation**"
      ],
      "metadata": {
        "id": "IqsQZa4SmJVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character_data.keys()"
      ],
      "metadata": {
        "id": "bE6PQNDdfSc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "character_data[216][\"max_proper_mention\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lSXIvZVJfVze",
        "outputId": "94b78eb5-df9d-40e0-894e-6ad5c3743cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Slughorn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def visualize_character_data(character_data, key):\n",
        "    # Extract data\n",
        "    data_list = character_data[key]\n",
        "    counts, items = zip(*data_list)\n",
        "    m = character_data[\"max_proper_mention\"]\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(items, counts, color='skyblue')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel(key.capitalize() if isinstance(key, str) else 'Data')\n",
        "    plt.title(f'Most Common {key.capitalize()} for {m}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "    print(m)\n",
        "\n",
        "visualize_character_data(character_data[216], 'possList')\n",
        "visualize_character_data(character_data[216], 'agentList')\n",
        "visualize_character_data(character_data[216], 'patientList')\n",
        "visualize_character_data(character_data[216], 'modList')\n"
      ],
      "metadata": {
        "id": "djvSYh3_gXch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Characters**"
      ],
      "metadata": {
        "id": "JfUML-IimaFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for num, character in list(character_data.items())[:10]:\n",
        "    print(f\"{num} - {character['max_proper_mention']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krJJhFYFiLod",
        "outputId": "2b5b42f6-7f86-405c-c637-bc673f360708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "136 - Harry\n",
            "146 - Dumbledore\n",
            "217 - Ron\n",
            "170 - Malfoy\n",
            "194 - Hermione\n",
            "358 - Hermione\n",
            "503 - Harry\n",
            "218 - Hagrid\n",
            "192 - Voldemort\n",
            "221 - Snape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract character IDs and maximum proper mentions\n",
        "character_ids = []\n",
        "max_proper_mentions = []\n",
        "for num, character in list(character_data.items())[:10]:\n",
        "    character_ids.append(num)\n",
        "    max_proper_mentions.append(character['max_proper_mention'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(max_proper_mentions, character_ids, color='skyblue')\n",
        "plt.xlabel('Character ID')\n",
        "plt.ylabel('Maximum Proper Mention')\n",
        "plt.title('Character IDs vs Maximum Proper Mentions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nfeP5go4iz5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = find_verb_usage(character_data)"
      ],
      "metadata": {
        "id": "Zp_EhyM7jXk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtdgdbF5jjw4",
        "outputId": "8d773b13-cc73-4d6d-e295-3243ef3480d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['agent', 'patient'])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verbs[\"agent\"][\"cried\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrUZddh7jiBC",
        "outputId": "8977bd1e-3623-4b46-9c4b-6af3569a2068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(333, 'Professor Trelawney'), (502, 'Dobby')]"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verbs[\"agent\"][\"knew\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSYAkv2qj2gn",
        "outputId": "7b2c02a5-c062-4d06-c578-d9aa8be6884a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(136, 'Harry'), (146, 'Dumbledore'), (503, 'Harry'), (192, 'Voldemort'), (221, 'Snape'), (379, 'Riddle'), (216, 'Slughorn'), (226, 'Tonks'), (144, 'Sirius'), (385, 'Stan Shunpike'), (542, 'Mr. Burke'), (367, 'Dumble - dore'), (479, 'Mal - foy')]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verbs[\"patient\"][\"asked\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFuRJW5j5hi",
        "outputId": "120330f4-2e00-4a82-aff8-1f4260e1cdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(136, 'Harry'), (146, 'Dumbledore'), (217, 'Ron'), (170, 'Malfoy'), (194, 'Hermione'), (358, 'Hermione'), (503, 'Harry'), (218, 'Hagrid'), (221, 'Snape'), (379, 'Riddle'), (236, 'Ginny'), (216, 'Slughorn'), (193, 'Slughorn'), (156, 'Snape'), (251, 'Professor McGonagall'), (227, 'Lupin'), (153, 'Katie'), (174, 'Neville'), (302, 'McLaggen'), (288, 'Luna'), (157, 'Bellatrix'), (220, 'Mr. Weasley'), (333, 'Professor Trelawney'), (241, 'Bill'), (232, 'Fred'), (419, 'Mrs. Cole'), (280, 'Borgin'), (301, 'Zabini'), (476, 'Filch'), (367, 'Dumble - dore'), (275, 'Michael Corner'), (461, 'Potty')]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"lemmas_final_harry.txt\""
      ],
      "metadata": {
        "id": "I_mXJuNFnb9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lemmas_final_harry.txt\", encoding='utf-8') as txt:\n",
        "    corpus = txt.read()"
      ],
      "metadata": {
        "id": "rIZ5RgRDn_G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}